# MLSS

# 2022-04-04
Lead Scribe - Chamudi


# A Survey of Methods for Explaining Black Box Models
Presented by Emmely
# Introduction

black box systems exploit sophisticated machine learning models to predict individual information that may also be sensitive

Machine learning algorithms build predictive models that are able to map user features into a class, from a learning phase

The learning process is made possible by the digital traces that people leave behind them while performing everyday activities 

By relying on sophisticated machine-learning classification models trained on massive datasets thanks to scalable, high-performance infrastructures, we risk to create and use decision systems that we do not really understand

This impacts not only information on ethics but also on accountability on safety, and on industrial liability

# NEEDS FOR INTERPRETABLE MODELS

briefly report some cases showing how and why black boxes can be dangerous

delegating decisions to black boxes without the possibility of an interpretation may be critical, can create discrimination and trust issues 



#  INTERPRETABLE, EXPLAINABLE, AND COMPREHENSIBLE MODELS

Understand the meaning of black box predictor and interpretability

A black box predictor is a data mining and machine learning obscure model, whose internals are either unknown to the observer or they are known but uninterpretable by humans

Inerpret - give or provide the meaning or to explain and present in understandable term some concepts.

another significant aspect to mention about interpretability is the reason why a system, a service or a method should be interpretable.


ex:-
surgeon who needs to remove 

# Dimensions of Interpretability

Global and Local Interpretability -  A model may be completely interpretable, (able to understand the whole logic of a model and follow the entire reasoning leading to all the different possible outcomes)


Time Limitation - An important aspect is the time that the user is available or is allowed to spend on understanding an explanation.

Nature of User Expertise - Users of a predictive model may have different background knowledge and experience in the task.

# Desiderata of an Interpretable Model

An interpretable model is required to provide an explanation.

it is necessary to consider the following list of desiderata

Interpretability- to which extent the model and/or its predictions are human understandable.

Accuracy-  to which extent the model accurately predicts unseen instances.

Fidelity - to which extent the model is able to accurately imitate a black-box predictor.

The degree of trust.  ona model increases if the model is built by repecting constraints of monotonicity given by the users.

# Recognized Interpretable Models

Small set of existing interpretable models is recognized: decision tree, rules, linear models.

These models are considered easily understandable and interpretable for humans

![](https://i.imgur.com/91vKHhO.png)

A decision system based on adecision tree exploits a graph structured like a tree and composed of internal nodes representing tests on features or attributes and leaf nodes representing a class label.

Each branch represents a possible outcome

The paths from the root to the leaves represent the
classification rules.

A decision rule is a function that maps an observation to an appropriate action.

Decision trees are widely adopted for their graphical representation, while rules have a textual representation

Textual representation does not provide immediately information about the more relevant attributes of a rule

The produced contributions summarize the performance of the model, thus the difference between the predictions of the model and expected predictions, providing the opportunity of quantifying the changes of the model prediction for each test record

 central problem of linear models -  when used for explanation is that when the model does not optimally fit the training data, it may use spurious features to optimize the error, and these features may be very hard to interpret for a human


# Explanations and Interpretable Models Complexity

The evaluation of the model complexity is generally tied to the model comprehensibility

evaluation is generally estimated with a rough approximation related to the size of the model

problem of black box explanation - the complexity is only related to the model and not to the training data that is generally unknown

Complexity is identified by the nuber of regions

Given two rules with similar frequency and accuracy, the rule with the smaller length may be preferred as it is more interpretable


# Interpretable Data for Interpretable Models

Different types of data present a different level of interpretability for a human

Majority of data mining and machine-learning techniques work on data organized in tables that algorithms may handle as matrices



# OPEN THE BLACK BOX PROBLEMS

Identification of different categories of problems
distinguish between reverse engineering and design of explanations

In the first case, given the decision records produced by a black box decision maker the problem consists in reconstructing an explanation for it

Different types of data present a different level of interpretability for a human


![](https://i.imgur.com/FgvIo4L.png)

Reverse Engineering: A Common Approach for Understanding the Black Box 

The black box predictor b is queried with a certain test dataset to create an oracle dataset that in turn will be used to train the comprehensible predictor

Through a deep analysis of the state of the art, we can further refine the first category into three different problems: 
* model explanation, 
* outcome explanation, 
* and model inspection

They name the first macro-category black box explanation problem and the second one transparent box design problem

![](https://i.imgur.com/9uEQVjB.png)



# Problem Formulation

Generalizing Classification problem


![](https://i.imgur.com/ZB0cZdL.png)

An instance xconsists of a set of m attribute-value pairs (ai,vi ), where ai is a feature (or attribute) and vi is a value from the domain of ai .

The domain of a feature can be continuous or categorical.

The target space Y (with dimensionality equals to one) contains the different labels (classes or outcomes) and also in this case the domain can be continuous or categorical. Note that, in case of ordinal classification labels in Y have an order. 

A predictor b can be a machine-learning model, a domainexpert rule-based system, or any combination of algorithmic and human knowledge processing.


# Model Explanation

The black box explanation problem consists in providing a global explanation of the black box model through an interpretable and transparent model.

The interpretable model approximating the black box must be globally interpretable


![](https://i.imgur.com/SAeAOZY.png)

Definition(Model Explanation Problem).

Given a black box predictor b and a set of instances X, the model explanation problem consists in finding an explanation E ‚àà E, belonging to a human interpretable domain E, through an interpretable global predictor c–¥ = f (b,X) derived from the black box b and the instances X using some process f (¬∑, ¬∑). An explanation E ‚àà E is obtained through c–¥, if E = Œµ–¥ (c–¥,X) for some explanation logic Œµ–¥ (¬∑, ¬∑), which reasons over c–¥ and X

# Outcome Explanation

Definition (Outcome Explanation Problem)

Given a black box predictor b and an instance x, the outcome explanation problem consists in finding an explanation e ‚àà E, belonging to a human interpretable domain E, through an interpretable local predictor cl = f (b, x) derived from the black box b and the instance x using some process f (¬∑, ¬∑). An explanation e ‚àà E is obtained through cl , if e = Œµl (cl , x) for some explanation logic Œµl (¬∑, ¬∑), which reasons over cl and x.


![](https://i.imgur.com/k3BDjd5.png)


# Model Inspection Problem

Definition (Model Inspection Problem)

Given a black box b and a set of instances X, the model inspection problem consists in providing a (visual or textual) representation r = f (b,X) of some property of b using some process f (¬∑, ¬∑).


example :- the function f may be based on sensitivity analysis that, by observing the changes occurring in the predictions when varying the input of b, returns a set of visualizations.

# Transparent Box Design Problem

The transparent box design problem consists in directly providing a model that is locally or globally interpretable.


Definition (Transparent Box Design Problem).

Given a training dataset D = {X,YÀÜ}, the transparent box design problem consists in learning a locally or globally interpretable predictor c from D. For a locally interpretable predictorc, there exists a local explanator logic Œµl to derive an explanation Œµl (c, x) of the decision c(x) for an instance x. For a globally interpretable predictorc, there exists a global explanator logic Œµ–¥ to derive an explanation Œµ–¥ (c,X).


![](https://i.imgur.com/6VoELH8.png)





![image](https://user-images.githubusercontent.com/28041333/163860227-192d4f68-6c4c-430f-b25e-2351da854da5.png)
